---
title: "STAT 4893W Research Code"
author: "Taarak Shah"
date: "4/3/2020"
output:
  word_document: default
  pdf_document: default
---

```{r}
#misc
library(class)
library(caret)
library(MASS)
library(tidyverse)
library(nnet)

#model selection
library(leaps)
#ROC / AUC
library(ROCR)
#trees
library(tree)
library(DAAG)
library(randomForest)
library(gbm)
```

# Data cleaning

```{r}
# import data
cyber = read.csv("data/data_postmerge.csv", header=TRUE, fileEncoding="UTF-8-BOM")
# convert target variable to categorical
cyber = cyber %>%
  mutate(score = case_when(
    score < 0.5                ~ "0",
    score >= 0.5 & score < 1.5 ~ "1",
    score >= 1.5 & score < 2.5 ~ "2",
    score >= 2.5 & score < 3.5 ~ "3",
    score >= 3.5 & score < 4.5 ~ "4",
    score >= 4.5 & score < 5.5 ~ "5",
    score >= 5.5 & score < 6.5 ~ "6",
    score >= 6.5 & score < 7.5 ~ "7",
    score >= 7.5 & score < 8.5 ~ "8",
    score >= 8.5 & score < 9.5 ~ "9",
    score >= 9.5               ~ "10",
))

# remove cve_id, pubdate, upd_date, auth (only has one level)
drop = c("cve_id","pubdate","upd_date", "auth")
cyber = cyber[,!(names(cyber) %in% drop)]
head(cyber)
```

```{r}
# fix data types, reorder factors

# gained_access: none, user, admin
# access = remote, local, localnet
# complexity: low, med, high
# confidentiality: none, partial, complete
# integrity: none, partial, complete
# availability: none, partial, complete
cyber$score = as.factor(cyber$score)
cyber$gained_access = factor(cyber$gained_access, levels(cyber$gained_access)[c(2,3,1)])
cyber$access = factor(cyber$access, levels(cyber$access)[c(3,1,2)])
cyber$complexity = factor(cyber$complexity, levels(cyber$complexity)[c(2,3,1)])
cyber$confidentiality = factor(cyber$confidentiality, levels(cyber$confidentiality)[c(2,3,1)])
cyber$integrity = factor(cyber$integrity, levels(cyber$integrity)[c(2,3,1)])
cyber$availability = factor(cyber$availability, levels(cyber$availability)[c(2,3,1)])

# remove rows with missing categorical vars
cyber = cyber[complete.cases(cyber), ]
head(cyber)
```

### Split test and train

```{r}
set.seed(4893)
n = length(cyber[,1])
index = sample(seq(1:n), 0.7*n)
train = cyber[index,]
test = cyber[-index,]
```

### Logistic regression

```{r}
mult.glm = multinom(score ~ gained_access + access + complexity + confidentiality + integrity + availability, data=train, family="binomial")
summary(mult.glm)

# best subset
#(bss.reg.sum = summary(regsubsets(score ~ gained_access + access + complexity + confidentiality + integrity + availability, data=train)))
#which.max(bss.reg.sum$adjr2)
# best includes all covariates

# deviance test
1-pchisq(mult.glm$deviance, mult.glm$edf, lower.tail = FALSE)
```

$H_0: l_\text{saturated} = l_\text{fitted}$

$H_a: l_\text{saturated} \neq l_\text{fitted}$

Fail to reject $H_0$, so then the fitted model is as good as the saturated model.

```{r}
pred.glm = predict(mult.glm, test, type="class")
glm.ER = sum(pred.glm != test$score) / length(test$score)
glm.ER

table.glm = table(test$score, pred.glm)
table.glm
```

# Decision trees

## Unpruned tree

```{r}
# decision tree, unpruned
groot1 = tree(score ~ gained_access + access + complexity + confidentiality + integrity + availability, data=train)
summary(groot1)
plot(groot1)
text(groot1, pretty = 0)
```

```{r}
# test error rate for unpruned tree
pred.tree = predict(groot1, newdata = test, type = "class")
table(test$score, pred.tree)
tree.ER1 = sum(pred.tree != test$score) / length(test$score)
tree.ER1
```

## Prune tree

```{r}
set.seed(4893)
groot2 <- cv.tree(groot1, FUN = prune.misclass)
plot(groot2$size, groot2$dev, type = "b")
groot3 = prune.misclass(groot1, best = 6)
plot(groot3)
text(groot3, pretty = 0)
```

```{r}
# test error rate for pruned tree
pred.tree2 = predict(groot3, newdata = test, type = "class")
table(test$score, pred.tree2)
tree.ER2 = sum(pred.tree2 != test$score) / length(test$score)
tree.ER2
```

Then use RF algorithm. Bagging will not be that useful since it will lead to correlated trees. Good RF default for classification tree is $m = \sqrt p$.

```{r}
set.seed(4893)
#random forest
rf = randomForest(score ~ gained_access + access + complexity + confidentiality + integrity + availability, data = train, mtry = 3, importance = TRUE)
pred.rf = predict(rf, newdata = test, type = "class")
table(test$score, pred.rf)
rf.ER = sum(pred.rf != test$score) / length(test$score)
rf.ER

importance(rf)
varImpPlot(rf)
```